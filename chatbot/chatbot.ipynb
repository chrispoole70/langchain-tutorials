{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzOa9TBbxP9fUGtBFxNSd8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chrispoole70/langchain-tutorials/blob/main/chatbot/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This tutorial is called [Build a Chatbot](https://python.langchain.com/docs/tutorials/chatbot/)"
      ],
      "metadata": {
        "id": "-P4b1Jsq1ZFc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ecNmxX2d1Wt3"
      },
      "outputs": [],
      "source": [
        "%pip install langchain-core langgraph>0.2.27"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU \"langchain[openai]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNKL84gi1bNw",
        "outputId": "82869136-6235-4d11-c83e-fde7f8035958"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/62.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m61.4/62.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.9/62.9 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Sequence\n",
        "from typing_extensions import TypedDict, Annotated\n",
        "\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, SystemMessage, trim_messages\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import StateGraph, MessagesState, START\n",
        "from langgraph.graph.message import add_messages"
      ],
      "metadata": {
        "id": "rhpt5XzH1_MR"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "adwqj-XI10gm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a chat model"
      ],
      "metadata": {
        "id": "HXMNX9aR2sqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model = init_chat_model(model='gpt-4o-mini', model_provider='openai')"
      ],
      "metadata": {
        "id": "_HjkeCr22ABv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a human message"
      ],
      "metadata": {
        "id": "9tSSAo8f3kBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "human_message = HumanMessage(content=\"Hi! I'm Bob.\")\n",
        "human_message"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnzQe6OJ2oTr",
        "outputId": "0a78b633-d883-4e92-d7f6-ae2abc442d0d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HumanMessage(content=\"Hi! I'm Bob.\", additional_kwargs={}, response_metadata={})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Send the human message to the chat model as input. In LangChain, chat models inherit the `Runnable` interface meaning you run the chat model by calling the `invoke()` method."
      ],
      "metadata": {
        "id": "KGWs8uao3oHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model.invoke([human_message])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZq3vbXH3PcK",
        "outputId": "7daecf1d-2de4-46c0-82ef-30191ed0d22d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hi Bob! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 12, 'total_tokens': 23, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0392822090', 'id': 'chatcmpl-BXALepRr0VjsRe7ECwDYsUzVtISSU', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--60475cba-6005-4028-8675-34cb6f2824b2-0', usage_metadata={'input_tokens': 12, 'output_tokens': 11, 'total_tokens': 23, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "But the chat model has no concept of state. That means asking a follow-up question won't work."
      ],
      "metadata": {
        "id": "bKjtnB6f4RBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model.invoke([HumanMessage(content=\"What's my name?\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMvZ6l9v4DzY",
        "outputId": "d6a2d1e4-9647-44e3-de14-38dabae86f30"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"I'm sorry, but I don't have access to personal information about you unless you've shared it during our conversation. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 11, 'total_tokens': 40, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0392822090', 'id': 'chatcmpl-BXANEKPL3Q7xgKDyd40MFI9gRQPiT', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--ffacb153-6aa2-495a-ba23-a5f16a136ca0-0', usage_metadata={'input_tokens': 11, 'output_tokens': 29, 'total_tokens': 40, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To be able to ask a follow-up question, you need to pass the conversation history to the chat model."
      ],
      "metadata": {
        "id": "Jp-tJuMc6oKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model.invoke(\n",
        "    [\n",
        "        HumanMessage(content=\"Hi! I'm Bob.\"),\n",
        "        AIMessage(content='Hi Bob! How can I assist you today?'),\n",
        "        HumanMessage(content=\"What's my name?\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iF6YRVQc4b4y",
        "outputId": "7509178c-03aa-483a-c9aa-64a4ad206353"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Your name is Bob. How can I help you today, Bob?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 34, 'total_tokens': 48, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0392822090', 'id': 'chatcmpl-BXAYH8TcLrH0MpRkhhu4Nn1ntU0Jb', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--9316d4e0-c686-4fd9-9398-59202aa338fe-0', usage_metadata={'input_tokens': 34, 'output_tokens': 14, 'total_tokens': 48, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Message Persistence"
      ],
      "metadata": {
        "id": "cBFzHiuH725V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One way to maintain the state is to use LangGraph. It can save the state to a database or in memory."
      ],
      "metadata": {
        "id": "Qfvf-g-qDR7L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To have a conversation in which the state is kept, first create a workflow for the chat model. This is also known as a graph."
      ],
      "metadata": {
        "id": "0Y5PBMLMD3Hw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "workflow = StateGraph(state_schema=MessagesState)"
      ],
      "metadata": {
        "id": "PEtlOuZu7DEL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each time input is sent to the chat model, this `call_model` function will be called. This will also add each message to a list of messages."
      ],
      "metadata": {
        "id": "1Zg6ZG8NEdSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def call_model(state: MessagesState):\n",
        "  response = chat_model.invoke(input=state['messages'])\n",
        "  return {'messages': response}"
      ],
      "metadata": {
        "id": "6oS1Ga0g9Eks"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build out the workflow by adding nodes and edges:\n",
        "* Edges are the steps in the application\n",
        "* Nodes are the connections between the steps"
      ],
      "metadata": {
        "id": "aoLSCieeC94J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "workflow.add_edge(START, 'model')\n",
        "workflow.add_node(node='model', action=call_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqqpDOPg9sEv",
        "outputId": "41e4283a-5ffb-45b9-eec0-c69dff0e28a1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7dc4b008e710>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile the workflow so that it saves the state to memory"
      ],
      "metadata": {
        "id": "FsRT71stH1Fq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)\n",
        "app"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "n3TjiA1yHxNn",
        "outputId": "22f805fe-396c-4097-9170-ae4008f3979d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.CompiledStateGraph object at 0x7dc4b008eb50>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`app` is a LangGraph application that also inherits `Runnable` meaning you can pass a message as input by calling `invoke()`. But you should also pass a `config` dictionary that includes a `thread_id` so that you can differentiate between conversations different users are having."
      ],
      "metadata": {
        "id": "_XgeJ9mAJqIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {'configurable': {'thread_id': 'abc123'}}\n",
        "\n",
        "query = \"Hi! I'm Bob.\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke(input={'messages': input_messages}, config=config)"
      ],
      "metadata": {
        "id": "MFmNoCubJdkv"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[message.pretty_print() for message in output['messages']]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sG6Os1qkKdlI",
        "outputId": "f0b1a5ae-434d-4523-e743-99ea8194a346"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Hi! I'm Bob.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hi Bob! How can I assist you today?\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can now ask follow-up questions"
      ],
      "metadata": {
        "id": "tl8-GWL9L8Xi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What's my name?\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\": input_messages}, config)"
      ],
      "metadata": {
        "id": "93OGuGDWLlXa"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[message.pretty_print() for message in output['messages']]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPdftiqQLpIa",
        "outputId": "6f79d9d2-513c-44f3-b32f-8be3aef07a33"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Hi! I'm Bob.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hi Bob! How can I assist you today?\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What's my name?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Your name is Bob! How can I help you today?\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None, None, None]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you change the `thread_id` then a new conversation will be started"
      ],
      "metadata": {
        "id": "b_vFwVIbM-c_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"abc234\"}}\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\": input_messages}, config)\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHNcqA7NMJ_g",
        "outputId": "2693410a-176d-4353-b59e-b3f82a297ffd"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "I don't have access to personal data about users unless it has been shared with me during our conversation. So, I don't know your name. If you'd like to share it, feel free!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Templates"
      ],
      "metadata": {
        "id": "0PizuHFjOQQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you want to provide a `SystemMessage` with instructions for the LLM, you can use a prompt template. Input can be passed to the prompt template using a `MessagesPlaceholder`."
      ],
      "metadata": {
        "id": "n9wTgjZFRnan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        ('system', 'You talk like a pirate. Answer all questions to the best of your ability.'),\n",
        "        MessagesPlaceholder(variable_name='messages')\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "m62Fpsd_NGGP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The prompt template is updated to replace the `MessagesPlaceholder` with the user input."
      ],
      "metadata": {
        "id": "3rTfrEwsU3Ln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "workflow = StateGraph(state_schema=MessagesState)\n",
        "\n",
        "def call_model(state: MessagesState):\n",
        "  prompt = prompt_template.invoke(state)\n",
        "  response = chat_model.invoke(prompt)\n",
        "  return {'messages': response}\n",
        "\n",
        "workflow.add_edge(start_key=START, end_key='model')\n",
        "workflow.add_node(node='model', action=call_model)\n",
        "\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "fdTHVClQT5rI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LangGraph app is invoked the same way as before"
      ],
      "metadata": {
        "id": "Y2HDz7VsVqLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"abc345\"}}\n",
        "query = \"Hi! I'm Jim.\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\": input_messages}, config)"
      ],
      "metadata": {
        "id": "KI2RhUXKVQbm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[message.pretty_print() for message in output['messages']]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lX0c4hl1VyeO",
        "outputId": "0928d825-0c4f-45e2-db99-df5da629acf4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Hi! I'm Jim.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Ahoy, Jim! What brings ye to this sea of words, matey? Speak yer mind, and I be here to lend a hand! Arrr!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is my name?\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\": input_messages}, config)\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YteMoXUmV30e",
        "outputId": "6eed3e44-6ec0-4b6c-dca9-ab591186b350"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Ye be callin' yerself Jim, if I recall correctly! A fine name for a hearty matey! Arrr! What else can I do fer ye?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also configure the prompt template to accept multiple inputs, such as the `SystemMessage` accepting an input called `language`"
      ],
      "metadata": {
        "id": "N_QZ2L0kZfwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "7FxbpGE7ZRO_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LangGraph app now has two parameters: `messages` and `language`. The state needs to be updated in order to represent this."
      ],
      "metadata": {
        "id": "ZDA5Te6zaFtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class State(TypedDict):\n",
        "  language: str\n",
        "  messages: Annotated[Sequence[BaseMessage], add_messages]"
      ],
      "metadata": {
        "id": "5rqR1PMuZ61Q"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "workflow = StateGraph(state_schema=State)\n",
        "\n",
        "def call_model(state: State):\n",
        "  prompt = prompt_template.invoke(state)\n",
        "  response = chat_model.invoke(prompt)\n",
        "  return {'messages': response}\n",
        "\n",
        "workflow.add_edge(start_key=START, end_key='model')\n",
        "workflow.add_node(node='model', action=call_model)\n",
        "\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "0BqaSwAmcXGS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can pass `language` and `messages` as input to the LangGraph app"
      ],
      "metadata": {
        "id": "lRmBPnqrgC-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"abc456\"}}\n",
        "query = \"Hi! I'm Bob.\"\n",
        "language = \"Spanish\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke(\n",
        "    {\"messages\": input_messages, \"language\": language},\n",
        "    config,\n",
        ")\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LPB1eu-f3BI",
        "outputId": "22e1b102-9b9d-4a1d-f10d-baa5f1321f7c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "¡Hola, Bob! ¿Cómo puedo ayudarte hoy?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can now omit passing `language` if that part of the state shouldn't change"
      ],
      "metadata": {
        "id": "UWr8rh6Oi7CH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is my name?\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke(\n",
        "    {\"messages\": input_messages},\n",
        "    config,\n",
        ")\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NR0ijQ11gLZo",
        "outputId": "15287182-bd3c-4b92-9fc8-73cc58c600f1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Tu nombre es Bob. ¿Hay algo más en lo que pueda ayudarte, Bob?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Managing Conversation History"
      ],
      "metadata": {
        "id": "1VVK3NoXjY9T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should limit the number of messages and/or their size. That way you keep costs down and don't exceed the LLM's context window. You can use the `trim_messages` function for this."
      ],
      "metadata": {
        "id": "W9AO9rIwl8CW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trimmer = trim_messages(\n",
        "    max_tokens=70,\n",
        "    strategy=\"last\",\n",
        "    token_counter=chat_model,\n",
        "    include_system=True,\n",
        "    allow_partial=False,\n",
        "    start_on=\"human\",\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"you're a good assistant\"),\n",
        "    HumanMessage(content=\"hi! I'm bob\"),\n",
        "    AIMessage(content=\"hi!\"),\n",
        "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
        "    AIMessage(content=\"nice\"),\n",
        "    HumanMessage(content=\"whats 2 + 2\"),\n",
        "    AIMessage(content=\"4\"),\n",
        "    HumanMessage(content=\"thanks\"),\n",
        "    AIMessage(content=\"no problem!\"),\n",
        "    HumanMessage(content=\"having fun?\"),\n",
        "    AIMessage(content=\"yes!\"),\n",
        "]\n",
        "\n",
        "trimmer.invoke(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sa_FG5rCjIXo",
        "outputId": "12bd20a3-f0c5-4e76-c36b-6c93b3634c8a"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps for trimming a conversation:\n",
        "1. Pass all the messages in the conversation to the `trimmer` to get the `trimmed_messages`.\n",
        "2. Update the `prompt_template` with the trimmed messages."
      ],
      "metadata": {
        "id": "OC7-odlSm87o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "workflow = StateGraph(state_schema=State)\n",
        "\n",
        "def call_model(state: State):\n",
        "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
        "    prompt = prompt_template.invoke(\n",
        "        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}\n",
        "    )\n",
        "    # Printing the prompt because there was an issue sending the human message \"whats 2 + 2\". Got around this by increasing the max number of tokens from 65 to 70.\n",
        "    print(f'Prompt: {prompt}')\n",
        "    response = chat_model.invoke(prompt)\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "workflow.add_edge(start_key=START, end_key='model')\n",
        "workflow.add_node(node='model', action=call_model)\n",
        "\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "oc7X9A_cm1I5"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because the beginning messages got trimmed, including the message \"hi! I'm bob\", they won't be sent to the chat model."
      ],
      "metadata": {
        "id": "UPCQZwgKoD4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"abc567\"}}\n",
        "query = \"What is my name?\"\n",
        "language = \"English\"\n",
        "\n",
        "input_messages = messages + [HumanMessage(query)]\n",
        "output = app.invoke(\n",
        "    {\"messages\": input_messages, \"language\": language},\n",
        "    config,\n",
        ")\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiBE1aQ1nx2I",
        "outputId": "53561552-85a9-414a-ecda-90581ad9a7d0"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: messages=[SystemMessage(content='You are a helpful assistant. Answer all questions to the best of your ability in English.', additional_kwargs={}, response_metadata={}), SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}, id='f01ca7e9-549b-4e36-bcdf-3cf553b12573'), HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}, id='66b89039-b620-431d-8209-2146e1f86aea'), AIMessage(content='4', additional_kwargs={}, response_metadata={}, id='f88b3a6a-7a19-4ea9-8232-f4020d9417ae'), HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}, id='4e451b2f-df59-401a-99a8-7507b25f0284'), AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}, id='75daec3f-a8b5-4e81-96e1-0c8f04dba26e'), HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}, id='d1083731-c2bb-4888-9aa3-8a639d38c184'), AIMessage(content='yes!', additional_kwargs={}, response_metadata={}, id='ee520018-913e-484a-9238-d7d795a43fa7'), HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}, id='3cba5ac8-cb65-4728-8168-011267aaf8c8')]\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "I don’t know your name. Would you like to share it?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "But the LLM can answer questions about the untrimmed messages in the conversation"
      ],
      "metadata": {
        "id": "8y_UqG_GokUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"abc678\"}}\n",
        "query = \"What math problem did I ask?\"\n",
        "language = \"English\"\n",
        "\n",
        "input_messages = messages + [HumanMessage(query)]\n",
        "output = app.invoke(\n",
        "    {\"messages\": input_messages, \"language\": language},\n",
        "    config,\n",
        ")\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDlGfDqNoWHW",
        "outputId": "29c97c57-5438-4920-d5fa-cff10b6f2267"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: messages=[SystemMessage(content='You are a helpful assistant. Answer all questions to the best of your ability in English.', additional_kwargs={}, response_metadata={}), SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}, id='f01ca7e9-549b-4e36-bcdf-3cf553b12573'), HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}, id='66b89039-b620-431d-8209-2146e1f86aea'), AIMessage(content='4', additional_kwargs={}, response_metadata={}, id='f88b3a6a-7a19-4ea9-8232-f4020d9417ae'), HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}, id='4e451b2f-df59-401a-99a8-7507b25f0284'), AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}, id='75daec3f-a8b5-4e81-96e1-0c8f04dba26e'), HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}, id='d1083731-c2bb-4888-9aa3-8a639d38c184'), AIMessage(content='yes!', additional_kwargs={}, response_metadata={}, id='ee520018-913e-484a-9238-d7d795a43fa7'), HumanMessage(content='What math problem did I ask?', additional_kwargs={}, response_metadata={}, id='a9e37f11-1892-47ae-861a-e572f57294f9')]\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "You asked the math problem \"What's 2 + 2?\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Uv6xJi4qony"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}