{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPr57C4Y1SfE8LsPk4aDn5h"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# [Build a simple LLM application with chat models and prompt templates](https://python.langchain.com/docs/tutorials/llm_chain/)"
      ],
      "metadata": {
        "id": "7B2YJKQYB7IH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mdgzo6Fs_18u"
      },
      "outputs": [],
      "source": [
        "%pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU \"langchain[openai]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUyJia_gB_yQ",
        "outputId": "5758daba-4c0e-4945-d4c1-95275a9d3b5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/437.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m317.4/437.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.6/437.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "LUaz8sWRJZUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "qywSv1AcJPhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the chat model"
      ],
      "metadata": {
        "id": "Qlpi718_NRg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
      ],
      "metadata": {
        "id": "-YgJ3ayQJdAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create input messages for the chat model"
      ],
      "metadata": {
        "id": "-qMJJYuvPS4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    SystemMessage(\"Translate the following from English into Italian\"),\n",
        "    HumanMessage(\"hi!\"),\n",
        "]"
      ],
      "metadata": {
        "id": "TYyYJwe5PVwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the chat model with the input messages to generate an output message synchronously"
      ],
      "metadata": {
        "id": "Yx9Ec-OtPydy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.invoke(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VClX5gFSP6Rq",
        "outputId": "7f33c80c-1c25-4215-8788-573cc3a109b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Ciao!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 20, 'total_tokens': 24, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0392822090', 'id': 'chatcmpl-BUEgdNvdH8fNH9ErBAw0JDNwXUzRv', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--e8f9b893-056b-4910-8e76-26206d57a58c-0', usage_metadata={'input_tokens': 20, 'output_tokens': 4, 'total_tokens': 24, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the chat model with the input messages to generate an output message asynchronously and print one output token at a time"
      ],
      "metadata": {
        "id": "Jxx18asrR10b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in model.stream(messages):\n",
        "  print(token.content, end=\"|\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MN-VeVeMQJZU",
        "outputId": "524ac96e-b28b-48f0-d386-6ecb6ecabdef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|C|iao|!||"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use a chat prompt template to take user input and create input messages with it. `system` and `user` are both roles that correspond to `SystemMessage` and `HumanMessage`."
      ],
      "metadata": {
        "id": "BcT6Pm8-TJn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_template = \"Translate the following from English into {language}\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
        ")"
      ],
      "metadata": {
        "id": "eQmHnei3Rapz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Format the prompt template with user input"
      ],
      "metadata": {
        "id": "f56GB9BmWOvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = prompt_template.invoke({\"language\": \"Italian\", \"text\": \"hi!\"})\n",
        "\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqJ_51qOTh-U",
        "outputId": "fa2404ca-7ea3-4ffc-f65a-c86277accbdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt.to_messages()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UsnuHXPWGtK",
        "outputId": "e26d75af-3d33-41cb-965a-be58a3c2917c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the model with the prompt template"
      ],
      "metadata": {
        "id": "j_LOmvF1WgkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.invoke(prompt)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YW7L_VHWLZK",
        "outputId": "1654453f-e858-47ce-e970-4f090157fec8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ciao!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GCOAnAU3X4wz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}