{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZeM1e+kJvSVdHIcx3+o5d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chrispoole70/langchain-tutorials/blob/main/extraction/extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Build an Extraction Chain](https://python.langchain.com/docs/tutorials/extraction/)"
      ],
      "metadata": {
        "id": "dJLxFUtBG52-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvOzXavmCyOo"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade langchain-core"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU \"langchain[openai]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSEjxWzLM7hQ",
        "outputId": "a0f1250a-7080-411a-fe58-1a87fbe48561"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.0 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pprint import pprint\n",
        "from typing import List, Optional\n",
        "\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.utils.function_calling import tool_example_to_messages\n",
        "from pydantic import BaseModel, Field"
      ],
      "metadata": {
        "id": "p8XhmBAsHhB_"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "3bYYc1kbNHJK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the schema for the information you want to extract from the unstructured data"
      ],
      "metadata": {
        "id": "tbMVPlnRJKWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Person(BaseModel):\n",
        "  \"\"\"Information about a person.\"\"\"\n",
        "\n",
        "  name: Optional[str] = Field(default=None, description='The name of the person')\n",
        "  hair_color: Optional[str] = Field(default=None, description='The color of the person\\'s hair if known')\n",
        "  height_in_meters: Optional[str] = Field(default=None, description='Height measured in meters')"
      ],
      "metadata": {
        "id": "PWAr0r_eIgEQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a prompt template"
      ],
      "metadata": {
        "id": "ndmHzbdPN2Na"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        ('system', 'You are an expert extraction algorithm. Only extract relevant information from the text. If you do not know the value of an attribute asked to extract, return null for the attribute\\'s value.'),\n",
        "        ('human', '{text}')\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "lxl5FfkTIpEF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a chat model and assign the output schema to it"
      ],
      "metadata": {
        "id": "JGvPLytJONFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = init_chat_model('gpt-4o-mini', model_provider='openai')"
      ],
      "metadata": {
        "id": "KdA8xPfwM_8A"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "structured_llm = llm.with_structured_output(schema=Person)"
      ],
      "metadata": {
        "id": "j9jwxbhGNe4g"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(structured_llm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "TRRsSsSQNv3x",
        "outputId": "c18aa1db-c63e-4e6b-b198-21f79367f1cd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain_core.runnables.base.RunnableSequence"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.runnables.base.RunnableSequence</b><br/>def __init__(*steps: RunnableLike, name: Optional[str]=None, first: Optional[Runnable[Any, Any]]=None, middle: Optional[list[Runnable[Any, Any]]]=None, last: Optional[Runnable[Any, Any]]=None) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py</a>Sequence of Runnables, where the output of each is the input of the next.\n",
              "\n",
              "**RunnableSequence** is the most important composition operator in LangChain\n",
              "as it is used in virtually every chain.\n",
              "\n",
              "A RunnableSequence can be instantiated directly or more commonly by using the `|`\n",
              "operator where either the left or right operands (or both) must be a Runnable.\n",
              "\n",
              "Any RunnableSequence automatically supports sync, async, batch.\n",
              "\n",
              "The default implementations of `batch` and `abatch` utilize threadpools and\n",
              "asyncio gather and will be faster than naive invocation of invoke or ainvoke\n",
              "for IO bound Runnables.\n",
              "\n",
              "Batching is implemented by invoking the batch method on each component of the\n",
              "RunnableSequence in order.\n",
              "\n",
              "A RunnableSequence preserves the streaming properties of its components, so if all\n",
              "components of the sequence implement a `transform` method -- which\n",
              "is the method that implements the logic to map a streaming input to a streaming\n",
              "output -- then the sequence will be able to stream input to output!\n",
              "\n",
              "If any component of the sequence does not implement transform then the\n",
              "streaming will only begin after this component is run. If there are\n",
              "multiple blocking components, streaming begins after the last one.\n",
              "\n",
              "Please note: RunnableLambdas do not support `transform` by default! So if\n",
              "    you need to use a RunnableLambdas be careful about where you place them in a\n",
              "    RunnableSequence (if you need to use the .stream()/.astream() methods).\n",
              "\n",
              "    If you need arbitrary logic and need streaming, you can subclass\n",
              "    Runnable, and implement `transform` for whatever logic you need.\n",
              "\n",
              "Here is a simple example that uses simple functions to illustrate the use of\n",
              "RunnableSequence:\n",
              "\n",
              "    .. code-block:: python\n",
              "\n",
              "        from langchain_core.runnables import RunnableLambda\n",
              "\n",
              "        def add_one(x: int) -&gt; int:\n",
              "            return x + 1\n",
              "\n",
              "        def mul_two(x: int) -&gt; int:\n",
              "            return x * 2\n",
              "\n",
              "        runnable_1 = RunnableLambda(add_one)\n",
              "        runnable_2 = RunnableLambda(mul_two)\n",
              "        sequence = runnable_1 | runnable_2\n",
              "        # Or equivalently:\n",
              "        # sequence = RunnableSequence(first=runnable_1, last=runnable_2)\n",
              "        sequence.invoke(1)\n",
              "        await sequence.ainvoke(1)\n",
              "\n",
              "        sequence.batch([1, 2, 3])\n",
              "        await sequence.abatch([1, 2, 3])\n",
              "\n",
              "Here&#x27;s an example that uses streams JSON output generated by an LLM:\n",
              "\n",
              "    .. code-block:: python\n",
              "\n",
              "        from langchain_core.output_parsers.json import SimpleJsonOutputParser\n",
              "        from langchain_openai import ChatOpenAI\n",
              "\n",
              "        prompt = PromptTemplate.from_template(\n",
              "            &#x27;In JSON format, give me a list of {topic} and their &#x27;\n",
              "            &#x27;corresponding names in French, Spanish and in a &#x27;\n",
              "            &#x27;Cat Language.&#x27;\n",
              "        )\n",
              "\n",
              "        model = ChatOpenAI()\n",
              "        chain = prompt | model | SimpleJsonOutputParser()\n",
              "\n",
              "        async for chunk in chain.astream({&#x27;topic&#x27;: &#x27;colors&#x27;}):\n",
              "            print(&#x27;-&#x27;)  # noqa: T201\n",
              "            print(chunk, sep=&#x27;&#x27;, flush=True)  # noqa: T201</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 2663);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(structured_llm.steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcaST_KuNxkV",
        "outputId": "e96f595d-af33-4f4b-cdd8-0eb17f1e9a0b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Format the prompt template with unstructured data about a person"
      ],
      "metadata": {
        "id": "fulPzsf6PAJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Alan Smith is 6 feet tall and has blond hair.'"
      ],
      "metadata": {
        "id": "jVk4pV51OcZZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = prompt_template.invoke({'text': text})\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oqq6T4W5O1vI",
        "outputId": "2507aa43-5ed8-4e8e-d2ed-664969d690ec"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content=\"You are an expert extraction algorithm. Only extract relevant information from the text. If you do not know the value of an attribute asked to extract, return null for the attribute's value.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Alan Smith is 6 feet tall and has blond hair.', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Send the input message to the LLM"
      ],
      "metadata": {
        "id": "pfwPeUSIPggF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = structured_llm.invoke(prompt)"
      ],
      "metadata": {
        "id": "sT47cVrjPGmP"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LLM was able to extract the person's name, hair color, and convert their height to meters"
      ],
      "metadata": {
        "id": "MZCWUMuePrTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_PXb9TjPjZB",
        "outputId": "faa116d0-8b1c-4d22-df7b-53e3db0c615f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Person(name='Alan Smith', hair_color='blond', height_in_meters='1.83')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiple Entities"
      ],
      "metadata": {
        "id": "aj6JskGDQL4I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In most cases your schema should be a list of elements that an LLM can extract info for. If there are multiple `People` in the unstructured data, than one way to extract this is to nest `People` inside another `Pydantic` class."
      ],
      "metadata": {
        "id": "RGX1v6JkQY8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Data(BaseModel):\n",
        "  \"\"\"Extracted data about people.\"\"\"\n",
        "\n",
        "  people: List[Person]"
      ],
      "metadata": {
        "id": "yfhqP8uRPmQA"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "structured_llm = llm.with_structured_output(schema=Data)"
      ],
      "metadata": {
        "id": "wKIhualSRhg6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use unstructured data about multiple people"
      ],
      "metadata": {
        "id": "Te8jiVvxRyLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'My name is Jeff, my hair is black and i am 6 feet tall. Anna has the same color hair as me.'"
      ],
      "metadata": {
        "id": "kLGpi9hXRuU4"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = prompt_template.invoke({'text': text})\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2527FCaVR3kv",
        "outputId": "2ec4df82-779e-4e89-e3c3-8626f4ffd26d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content=\"You are an expert extraction algorithm. Only extract relevant information from the text. If you do not know the value of an attribute asked to extract, return null for the attribute's value.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='My name is Jeff, my hair is black and i am 6 feet tall. Anna has the same color hair as me.', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = structured_llm.invoke(prompt)"
      ],
      "metadata": {
        "id": "cufPKSqHR-qs"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output will have info about two people"
      ],
      "metadata": {
        "id": "t62kjlNoSRm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-uQraXISKtC",
        "outputId": "908e854f-88dd-4bde-f7e1-54cb23481637"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(people=[Person(name='Jeff', hair_color='black', height_in_meters='1.83'), Person(name='Anna', hair_color='black', height_in_meters=None)])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reference Examples"
      ],
      "metadata": {
        "id": "1TaTv1vhVqnl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Give the LLM examples of input and their expected output. One way of sending multiple input/output combinations to an LLM is using the `tool_example_to_messages` function which states whether a tool call was made correctly or not."
      ],
      "metadata": {
        "id": "n_os2XhkdUXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    (\n",
        "        'The ocean is vast and blue. It\\'s more than 20,000 feet deep.',\n",
        "        Data(people=[])\n",
        "    ),\n",
        "    (\n",
        "        'Fiona traveled far from France to Spain.',\n",
        "        Data(people=[Person(name='Fiona', hair_color=None, height_in_meters=None)])\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "id": "EWmDlJ6DalZx"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for txt, tool_call in examples:\n",
        "  print(f'Text: {txt}')\n",
        "  print(f'Tool call: {tool_call}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VegqTCMebFqs",
        "outputId": "3a823598-c29f-40b7-f3f4-641dafa2cf62"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: The ocean is vast and blue. It's more than 20,000 feet deep.\n",
            "Tool call: people=[]\n",
            "Text: Fiona traveled far from France to Spain.\n",
            "Tool call: people=[Person(name='Fiona', hair_color=None, height_in_meters=None)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = []\n",
        "\n",
        "for txt, tool_call in examples:\n",
        "  if tool_call.people:\n",
        "    ai_response = 'Detected people.'\n",
        "  else:\n",
        "    ai_response = 'Detected no people.'\n",
        "\n",
        "  messages.extend(tool_example_to_messages(input=txt, tool_calls=[tool_call], ai_response=ai_response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hcD8rambUYE",
        "outputId": "f7408005-9d28-4a3e-88cd-f00e90327f21"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-b81934c082e1>:9: LangChainBetaWarning: The function `tool_example_to_messages` is in beta. It is actively being worked on, so the API may change.\n",
            "  messages.extend(tool_example_to_messages(input=txt, tool_calls=[tool_call], ai_response=ai_response))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for message in messages:\n",
        "  pprint(message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhIeKLHEcoKY",
        "outputId": "930fb4e2-6fc2-48b5-e34f-cc5e2aa53947"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HumanMessage(content=\"The ocean is vast and blue. It's more than 20,000 feet deep.\", additional_kwargs={}, response_metadata={})\n",
            "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '17968c68-f73f-4d20-be5e-6353e7c7b7f3', 'type': 'function', 'function': {'name': 'Data', 'arguments': '{\"people\":[]}'}}]}, response_metadata={}, tool_calls=[{'name': 'Data', 'args': {'people': []}, 'id': '17968c68-f73f-4d20-be5e-6353e7c7b7f3', 'type': 'tool_call'}])\n",
            "ToolMessage(content='You have correctly called this tool.', tool_call_id='17968c68-f73f-4d20-be5e-6353e7c7b7f3')\n",
            "AIMessage(content='Detected no people.', additional_kwargs={}, response_metadata={})\n",
            "HumanMessage(content='Fiona traveled far from France to Spain.', additional_kwargs={}, response_metadata={})\n",
            "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '74cc3254-f739-4dea-addf-c92f2e786301', 'type': 'function', 'function': {'name': 'Data', 'arguments': '{\"people\":[{\"name\":\"Fiona\",\"hair_color\":null,\"height_in_meters\":null}]}'}}]}, response_metadata={}, tool_calls=[{'name': 'Data', 'args': {'people': [{'name': 'Fiona', 'hair_color': None, 'height_in_meters': None}]}, 'id': '74cc3254-f739-4dea-addf-c92f2e786301', 'type': 'tool_call'}])\n",
            "ToolMessage(content='You have correctly called this tool.', tool_call_id='74cc3254-f739-4dea-addf-c92f2e786301')\n",
            "AIMessage(content='Detected people.', additional_kwargs={}, response_metadata={})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sending input to an LLM without examples sometimes returns an incorrect output. In this text, the LLM sometimes confuses Earth as a person's name."
      ],
      "metadata": {
        "id": "WkR41gefegCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message_no_extraction = {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"The solar system is large, but earth has only 1 moon.\",\n",
        "}\n",
        "\n",
        "structured_llm = llm.with_structured_output(schema=Data)\n",
        "structured_llm.invoke([message_no_extraction])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmzdpZbzcuxG",
        "outputId": "0f0bd78d-4f21-4039-f5b3-60332361fde2"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(people=[])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By including examples of correct input/output combinations, the LLM should become more accurate"
      ],
      "metadata": {
        "id": "66pdUqF2fIOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "structured_llm.invoke(messages + [message_no_extraction])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZW581axevkH",
        "outputId": "d1871d89-67ad-4974-98b2-6e2fe110ac29"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(people=[])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ca3G_01ufbYE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}