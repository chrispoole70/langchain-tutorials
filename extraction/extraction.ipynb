{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqi/nf+8D3g6he22ui88A9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chrispoole70/langchain-tutorials/blob/main/extraction/extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Build an Extraction Chain](https://python.langchain.com/docs/tutorials/extraction/)"
      ],
      "metadata": {
        "id": "dJLxFUtBG52-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvOzXavmCyOo"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade langchain-core"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU \"langchain[openai]\""
      ],
      "metadata": {
        "id": "iSEjxWzLM7hQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pprint import pprint\n",
        "from typing import List, Optional\n",
        "\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.utils.function_calling import tool_example_to_messages\n",
        "from pydantic import BaseModel, Field"
      ],
      "metadata": {
        "id": "p8XhmBAsHhB_"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "3bYYc1kbNHJK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the schema for the information you want to extract from the unstructured data"
      ],
      "metadata": {
        "id": "tbMVPlnRJKWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Person(BaseModel):\n",
        "  \"\"\"Information about a person.\"\"\"\n",
        "\n",
        "  name: Optional[str] = Field(default=None, description='The name of the person')\n",
        "  hair_color: Optional[str] = Field(default=None, description='The color of the person\\'s hair if known')\n",
        "  height_in_meters: Optional[str] = Field(default=None, description='Height measured in meters')"
      ],
      "metadata": {
        "id": "PWAr0r_eIgEQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a prompt template"
      ],
      "metadata": {
        "id": "ndmHzbdPN2Na"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        ('system', 'You are an expert extraction algorithm. Only extract relevant information from the text. If you do not know the value of an attribute asked to extract, return null for the attribute\\'s value.'),\n",
        "        ('human', '{text}')\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "lxl5FfkTIpEF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a chat model and assign the output schema to it"
      ],
      "metadata": {
        "id": "JGvPLytJONFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = init_chat_model('gpt-4o-mini', model_provider='openai')"
      ],
      "metadata": {
        "id": "KdA8xPfwM_8A"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "structured_llm = llm.with_structured_output(schema=Person)"
      ],
      "metadata": {
        "id": "j9jwxbhGNe4g"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(structured_llm).__name__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TRRsSsSQNv3x",
        "outputId": "2431cf8a-c396-4315-b7f8-74426e6d03e2"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'RunnableSequence'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(structured_llm.steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcaST_KuNxkV",
        "outputId": "e96f595d-af33-4f4b-cdd8-0eb17f1e9a0b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Format the prompt template with unstructured data about a person"
      ],
      "metadata": {
        "id": "fulPzsf6PAJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Alan Smith is 6 feet tall and has blond hair.'"
      ],
      "metadata": {
        "id": "jVk4pV51OcZZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = prompt_template.invoke({'text': text})\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oqq6T4W5O1vI",
        "outputId": "2507aa43-5ed8-4e8e-d2ed-664969d690ec"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content=\"You are an expert extraction algorithm. Only extract relevant information from the text. If you do not know the value of an attribute asked to extract, return null for the attribute's value.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Alan Smith is 6 feet tall and has blond hair.', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Send the input message to the LLM"
      ],
      "metadata": {
        "id": "pfwPeUSIPggF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = structured_llm.invoke(prompt)"
      ],
      "metadata": {
        "id": "sT47cVrjPGmP"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LLM was able to extract the person's name, hair color, and convert their height to meters"
      ],
      "metadata": {
        "id": "MZCWUMuePrTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_PXb9TjPjZB",
        "outputId": "faa116d0-8b1c-4d22-df7b-53e3db0c615f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Person(name='Alan Smith', hair_color='blond', height_in_meters='1.83')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiple Entities"
      ],
      "metadata": {
        "id": "aj6JskGDQL4I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In most cases your schema should be a list of elements that an LLM can extract info for. If there are multiple `People` in the unstructured data, than one way to extract this is to nest `People` inside another `Pydantic` class."
      ],
      "metadata": {
        "id": "RGX1v6JkQY8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Data(BaseModel):\n",
        "  \"\"\"Extracted data about people.\"\"\"\n",
        "\n",
        "  people: List[Person]"
      ],
      "metadata": {
        "id": "yfhqP8uRPmQA"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "structured_llm = llm.with_structured_output(schema=Data)"
      ],
      "metadata": {
        "id": "wKIhualSRhg6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use unstructured data about multiple people"
      ],
      "metadata": {
        "id": "Te8jiVvxRyLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'My name is Jeff, my hair is black and i am 6 feet tall. Anna has the same color hair as me.'"
      ],
      "metadata": {
        "id": "kLGpi9hXRuU4"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = prompt_template.invoke({'text': text})\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2527FCaVR3kv",
        "outputId": "2ec4df82-779e-4e89-e3c3-8626f4ffd26d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content=\"You are an expert extraction algorithm. Only extract relevant information from the text. If you do not know the value of an attribute asked to extract, return null for the attribute's value.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='My name is Jeff, my hair is black and i am 6 feet tall. Anna has the same color hair as me.', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = structured_llm.invoke(prompt)"
      ],
      "metadata": {
        "id": "cufPKSqHR-qs"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output will have info about two people"
      ],
      "metadata": {
        "id": "t62kjlNoSRm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-uQraXISKtC",
        "outputId": "908e854f-88dd-4bde-f7e1-54cb23481637"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(people=[Person(name='Jeff', hair_color='black', height_in_meters='1.83'), Person(name='Anna', hair_color='black', height_in_meters=None)])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reference Examples"
      ],
      "metadata": {
        "id": "1TaTv1vhVqnl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Give the LLM examples of input and their expected output. One way of sending multiple input/output combinations to an LLM is using the `tool_example_to_messages` function which states whether a tool call was made correctly or not."
      ],
      "metadata": {
        "id": "n_os2XhkdUXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    (\n",
        "        'The ocean is vast and blue. It\\'s more than 20,000 feet deep.',\n",
        "        Data(people=[])\n",
        "    ),\n",
        "    (\n",
        "        'Fiona traveled far from France to Spain.',\n",
        "        Data(people=[Person(name='Fiona', hair_color=None, height_in_meters=None)])\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "id": "EWmDlJ6DalZx"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for txt, tool_call in examples:\n",
        "  print(f'Text: {txt}')\n",
        "  print(f'Tool call: {tool_call}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VegqTCMebFqs",
        "outputId": "3a823598-c29f-40b7-f3f4-641dafa2cf62"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: The ocean is vast and blue. It's more than 20,000 feet deep.\n",
            "Tool call: people=[]\n",
            "Text: Fiona traveled far from France to Spain.\n",
            "Tool call: people=[Person(name='Fiona', hair_color=None, height_in_meters=None)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = []\n",
        "\n",
        "for txt, tool_call in examples:\n",
        "  if tool_call.people:\n",
        "    ai_response = 'Detected people.'\n",
        "  else:\n",
        "    ai_response = 'Detected no people.'\n",
        "\n",
        "  messages.extend(tool_example_to_messages(input=txt, tool_calls=[tool_call], ai_response=ai_response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hcD8rambUYE",
        "outputId": "f7408005-9d28-4a3e-88cd-f00e90327f21"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-b81934c082e1>:9: LangChainBetaWarning: The function `tool_example_to_messages` is in beta. It is actively being worked on, so the API may change.\n",
            "  messages.extend(tool_example_to_messages(input=txt, tool_calls=[tool_call], ai_response=ai_response))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for message in messages:\n",
        "  pprint(message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhIeKLHEcoKY",
        "outputId": "930fb4e2-6fc2-48b5-e34f-cc5e2aa53947"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HumanMessage(content=\"The ocean is vast and blue. It's more than 20,000 feet deep.\", additional_kwargs={}, response_metadata={})\n",
            "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '17968c68-f73f-4d20-be5e-6353e7c7b7f3', 'type': 'function', 'function': {'name': 'Data', 'arguments': '{\"people\":[]}'}}]}, response_metadata={}, tool_calls=[{'name': 'Data', 'args': {'people': []}, 'id': '17968c68-f73f-4d20-be5e-6353e7c7b7f3', 'type': 'tool_call'}])\n",
            "ToolMessage(content='You have correctly called this tool.', tool_call_id='17968c68-f73f-4d20-be5e-6353e7c7b7f3')\n",
            "AIMessage(content='Detected no people.', additional_kwargs={}, response_metadata={})\n",
            "HumanMessage(content='Fiona traveled far from France to Spain.', additional_kwargs={}, response_metadata={})\n",
            "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '74cc3254-f739-4dea-addf-c92f2e786301', 'type': 'function', 'function': {'name': 'Data', 'arguments': '{\"people\":[{\"name\":\"Fiona\",\"hair_color\":null,\"height_in_meters\":null}]}'}}]}, response_metadata={}, tool_calls=[{'name': 'Data', 'args': {'people': [{'name': 'Fiona', 'hair_color': None, 'height_in_meters': None}]}, 'id': '74cc3254-f739-4dea-addf-c92f2e786301', 'type': 'tool_call'}])\n",
            "ToolMessage(content='You have correctly called this tool.', tool_call_id='74cc3254-f739-4dea-addf-c92f2e786301')\n",
            "AIMessage(content='Detected people.', additional_kwargs={}, response_metadata={})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sending input to an LLM without examples sometimes returns an incorrect output. In this text, the LLM sometimes confuses Earth as a person's name."
      ],
      "metadata": {
        "id": "WkR41gefegCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message_no_extraction = {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"The solar system is large, but earth has only 1 moon.\",\n",
        "}\n",
        "\n",
        "structured_llm = llm.with_structured_output(schema=Data)\n",
        "structured_llm.invoke([message_no_extraction])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmzdpZbzcuxG",
        "outputId": "0f0bd78d-4f21-4039-f5b3-60332361fde2"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(people=[])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By including examples of correct input/output combinations, the LLM should become more accurate"
      ],
      "metadata": {
        "id": "66pdUqF2fIOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "structured_llm.invoke(messages + [message_no_extraction])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZW581axevkH",
        "outputId": "d1871d89-67ad-4974-98b2-6e2fe110ac29"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(people=[])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ca3G_01ufbYE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}